#!/bin/bash

# This script fixes the circular import error by moving custom exceptions
# into their own dedicated file within the core module.

echo "--- Fixing Circular Import: Creating core/exceptions.py ---"

# 1. Create the new exceptions file.
cat <<'EOF' > ./src/mlx_rl_trainer/core/exceptions.py
"""
Custom exception classes for the MLX RL Trainer framework.
This module exists to prevent circular import errors between core components.
"""

class CustomBaseException(Exception):
    """Base exception for all custom errors in this project."""
    pass

class ModelLoadError(CustomBaseException):
    """Raised when a model fails to load."""
    pass

class DataLoadError(CustomBaseException):
    """Raised when a dataset fails to load or process."""
    pass

class CheckpointError(CustomBaseException):
    """Raised for errors during checkpoint saving or loading."""
    pass

class InvalidConfigurationError(CustomBaseException):
    """Raised when a configuration is invalid or missing required fields."""
    pass

class TrainingRuntimeError(CustomBaseException):
    """Raised for general errors that occur during the training loop."""
    pass
EOF

echo "--- Updating core/trainer.py to use new exceptions module ---"

# 2. Update trainer.py to import exceptions instead of defining them.
cat <<'EOF' > ./src/mlx_rl_trainer/core/trainer.py
"""Base trainer interface and shared training abstractions."""
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import Dict, Any, Optional, List, Tuple, Callable
import logging, time, gc
import mlx.core as mx, mlx.nn as nn, mlx.optimizers as optim
import numpy as np
from tqdm import trange

from .config import ExperimentConfig
from .model_manager import ModelManager
from .dataset_manager import DatasetManager
from .checkpoint_manager import CheckpointManager
from ..monitoring.metrics_logger import MetricsLogger
from .exceptions import TrainingRuntimeError, CheckpointError # Import from the new module

logger = logging.getLogger(__name__)

@dataclass(frozen=True)
class TrainingMetrics:
    loss: float
    reward_mean: float
    grad_norm: float
    learning_rate: float
    step_time_s: float
    kl_divergence: float
    epoch: int = 0
    step: int = 0
    reward_std: float = 0.0
    custom_metrics: Dict[str, float] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
        data = {
            "train/loss": self.loss,
            "train/reward_mean": self.reward_mean,
            "train/reward_std": self.reward_std,
            "train/grad_norm": self.grad_norm,
            "train/learning_rate": self.learning_rate,
            "train/step_time_s": self.step_time_s,
            "train/kl_divergence": self.kl_divergence,
            "train/epoch": self.epoch,
            "train/step": self.step,
        }
        data.update(self.custom_metrics)
        return data

@dataclass(frozen=True)
class EvaluationMetrics:
    task_name: str
    pass_rate: float = 0.0
    perplexity: Optional[float] = None
    additional_info: Dict[str, Any] = field(default_factory=dict)
    def to_dict(self) -> Dict[str, Any]:
        data = {f"eval/{self.task_name}/pass_rate": self.pass_rate}
        if self.perplexity is not None: data[f"eval/{self.task_name}/perplexity"] = self.perplexity
        for k, v in self.additional_info.items():
            if not k.startswith(f"eval/{self.task_name}/"): data[f"eval/{self.task_name}/{k}"] = v
            else: data[k] = v
        return data

class BaseTrainer(ABC):
    def __init__( self, config: ExperimentConfig, model_manager: ModelManager, data_manager: DatasetManager,
                  checkpoint_manager: CheckpointManager, reward_composer: Any, paged_kv_cache: Optional[Any], metrics_logger: Optional[MetricsLogger] = None):
        self.config, self.model_manager, self.data_manager, self.checkpoint_manager, self.reward_composer, self.paged_kv_cache, self.metrics_logger = \
            config, model_manager, data_manager, checkpoint_manager, reward_composer, paged_kv_cache, metrics_logger
        self.actor_model, self.ref_model, self.tokenizer, self.optimizer, self.lr_scheduler = None, None, None, None, None
        self.global_step, self.current_epoch = 0, 0
        self._run_id = self.metrics_logger.run_id if self.metrics_logger else f"run_{time.strftime('%Y%m%d-%H%M%S')}"
        logger.info("BaseTrainer initialized.")

    @abstractmethod
    def _setup(self) -> Tuple[int, int]: raise NotImplementedError
    @abstractmethod
    def train_step(self, rollout_batch: Dict[str, mx.array], update_step: int) -> Tuple[TrainingMetrics, Dict[str, mx.array]]: raise NotImplementedError
    @abstractmethod
    def generate_rollouts(self, batch_data: Dict[str, Any], update_step: int) -> Tuple[Dict[str, mx.array], float, Dict[str, float]]: raise NotImplementedError
    @abstractmethod
    def evaluate(self, update_step: int) -> List[EvaluationMetrics]: raise NotImplementedError

    def save_final_checkpoint(self, reason: str = "final"):
        if self.actor_model:
            self.checkpoint_manager.save_checkpoint(
                step=self.global_step, model=self.actor_model, optimizer=self.optimizer,
                metadata={"num_updates": self.global_step, "epoch": self.current_epoch, "reason": reason, "log_id": self._run_id, "save_optimizer_state": self.config.checkpointing.save_optimizer_state},
                current_metric=self.checkpoint_manager.best_metric
            )

    async def run(self, should_shutdown: Callable[[], bool]):
        self.global_step, self.current_epoch = self._setup()
        
        if self.tokenizer:
            self.data_manager.set_tokenizer(self.tokenizer)
            self.data_manager.set_system_prompt(self.config.system_prompt)

        await self.data_manager.load_datasets()
        
        pbar = trange(self.global_step, self.config.trainer.num_training_steps, initial=self.global_step, desc="Training Progress", unit="update", leave=True)
        train_data_iterator = iter(self.data_manager.get_dataloader("train", self.config.trainer.ppo_batch_size))
        
        with pbar:
            while self.global_step < self.config.trainer.num_training_steps:
                if should_shutdown():
                    logger.info("Shutdown requested. Breaking training loop.")
                    self.save_final_checkpoint(reason="signal")
                    break

                accumulated_metrics_list, avg_rewards_list, raw_reward_components_list = [], [], []
                accum_grads = None

                for _ in range(self.config.trainer.grad_accum_steps):
                    try: batch_data = next(train_data_iterator)
                    except StopIteration:
                        self.current_epoch += 1; logger.info(f"Starting Epoch {self.current_epoch}")
                        train_data_iterator = iter(self.data_manager.get_dataloader("train", self.config.trainer.ppo_batch_size))
                        try: batch_data = next(train_data_iterator)
                        except StopIteration: raise TrainingRuntimeError("Dataset exhausted.")
                    
                    rollout_batch, avg_reward_mb, raw_reward_components_mb = self.generate_rollouts(batch_data, self.global_step)
                    
                    if not rollout_batch or 'tokens' not in rollout_batch or not isinstance(rollout_batch['tokens'], mx.array) or rollout_batch['tokens'].size == 0:
                        logger.warning(f"Micro-batch at step {self.global_step} produced no valid rollouts. Skipping.")
                        continue

                    metrics_mb, grads_mb = self.train_step(rollout_batch, self.global_step)
                    accumulated_metrics_list.append(metrics_mb)
                    avg_rewards_list.append(avg_reward_mb)
                    raw_reward_components_list.append(raw_reward_components_mb)
                    
                    if grads_mb: accum_grads = mx.utils.tree_map(mx.add, accum_grads, grads_mb) if accum_grads else grads_mb
                    mx.clear_cache(); gc.collect()

                if accum_grads and self.optimizer:
                     grad_norm = np.linalg.norm([np.linalg.norm(v.flatten()) for v in mx.utils.tree_flatten(accum_grads)[1]])
                     
                     self.optimizer.set_learning_rate(mx.array(float(self.lr_scheduler(self.global_step))))
                     self.optimizer.apply_gradients(accum_grads, self.actor_model.trainable_parameters())
                     mx.eval(self.actor_model.parameters(), self.optimizer.state)
                
                     avg_loss = np.mean([m.loss for m in accumulated_metrics_list])
                     avg_reward_mean = np.mean(avg_rewards_list)
                     avg_lr = self.lr_scheduler(self.global_step)
                     aggregated_raw_rewards = {k: np.mean([comp.get(k, 0.0) for comp in raw_reward_components_list]) for k in raw_reward_components_list[0]} if raw_reward_components_list else {}

                     if self.metrics_logger:
                         self.metrics_logger.log_metrics({
                            "train/loss": avg_loss, "train/reward_mean": avg_reward_mean, "train/grad_norm": grad_norm,
                            "train/learning_rate": avg_lr, "train/kl_divergence": np.mean([m.kl_divergence for m in accumulated_metrics_list]),
                            "train/epoch": self.current_epoch, "train/step": self.global_step,
                            **{f"train/rewards/raw_{k}": v for k,v in aggregated_raw_rewards.items()}
                         }, step=self.global_step)

                     pbar.set_postfix({"Loss": f"{avg_loss:.4f}", "Rew": f"{avg_reward_mean:.3f}", "LR": f"{avg_lr:.1e}", "GradN": f"{grad_norm:.3f}"})
                     pbar.update(1)

                     is_eval = (self.config.trainer.eval_every > 0 and (self.global_step + 1) % self.config.trainer.eval_every == 0)
                     is_save = (self.config.checkpointing.save_every > 0 and (self.global_step + 1) % self.config.checkpointing.save_every == 0)
                     is_final = (self.global_step == self.config.trainer.num_training_steps - 1)
                     
                     primary_metric = -float("inf")
                     if is_eval or is_final:
                         eval_results = self.evaluate(self.global_step)
                         for metric in eval_results:
                             if self.metrics_logger: self.metrics_logger.log_metrics(metric.to_dict(), step=self.global_step)
                             if metric.pass_rate > primary_metric: primary_metric = metric.pass_rate

                     should_save_best = self.checkpoint_manager.is_best_metric(primary_metric)
                     if is_save or is_final or should_save_best:
                          self.checkpoint_manager.save_checkpoint(
                              step=self.global_step, model=self.actor_model, optimizer=self.optimizer,
                              metadata={"num_updates": self.global_step, "epoch": self.current_epoch, "save_optimizer_state": self.config.checkpointing.save_optimizer_state},
                              current_metric=primary_metric 
                          )
                
                self.global_step += 1
            
            self.save_final_checkpoint(reason="completed" if not should_shutdown() else "interrupted")
EOF

echo "--- Updating core/checkpoint_manager.py to use new exceptions module ---"

# 3. Update checkpoint_manager.py to import from the new exceptions file.
cat <<'EOF' > ./src/mlx_rl_trainer/core/checkpoint_manager.py
"""Checkpoint management for training persistence."""
import json, logging, os, re, shutil, time
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple
import mlx.core as mx, mlx.nn as nn, mlx.optimizers as optim
from mlx.utils import tree_flatten, tree_unflatten
from .exceptions import CheckpointError
from rich import print as rprint
import random, numpy as np
from mlx_lm.tuner.utils import remove_lora_layers
from mlx_lm.utils import save_config

try:
    from mlx_lm.tuner.lora import LoRALinear as MLXLoRALinear
except ImportError:
    class MLXLoRALinear: pass

logger = logging.getLogger(__name__)

class CheckpointManager:
    """Manages checkpoint lifecycle: saving, loading, and rotation."""
    def __init__(self, output_dir: Path, keep_last_n: int = 3, save_best: bool = True):
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.keep_last_n = keep_last_n
        self.save_best = save_best
        self.best_metric: float = -float("inf")
        self._checkpoints: List[Path] = []
        self.resume_from_path: Optional[Path] = None
        self._load_existing_checkpoints()

    def _load_existing_checkpoints(self):
        checkpoint_regex = re.compile(r"checkpoint_[\d]{8}_[\d]{6}_update_(\d+)$")
        found_dirs_with_steps: List[Tuple[int, Path]] = []
        
        for p in self.output_dir.iterdir():
            if p.is_dir() and (p / "metadata.json").is_file():
                if match := checkpoint_regex.match(p.name):
                    found_dirs_with_steps.append((int(match.group(1)), p))
        
        found_dirs_with_steps.sort(key=lambda x: x[0])
        self._checkpoints = [p for _, p in found_dirs_with_steps]
        
        best_symlink = self.output_dir / "best"
        if best_symlink.is_symlink() and best_symlink.resolve().is_dir():
            try:
                with open(best_symlink.resolve() / "metadata.json", "r") as f:
                    self.best_metric = json.load(f).get("current_metric", -float("inf"))
            except Exception as e:
                logging.warning(f"Could not load best_metric from 'best' symlink: {e}")

    def save_checkpoint(self, step: int, model: nn.Module, optimizer: optim.Optimizer, metadata: Dict[str, Any], current_metric: float):
        timestamp = time.strftime('%Y%m%d_%H%M%S')
        checkpoint_name = f"checkpoint_{timestamp}_update_{step}"
        temp_path = self.output_dir / f".{checkpoint_name}.tmp"
        final_path = self.output_dir / checkpoint_name
        shutil.rmtree(temp_path, ignore_errors=True)
        
        try:
            temp_path.mkdir(parents=True)
            
            is_lora = any(isinstance(m, MLXLoRALinear) for _, m in model.named_modules())
            if is_lora:
                adapter_params = dict(tree_flatten(model.trainable_parameters()))
                if adapter_params: mx.save_safetensors(str(temp_path / "adapters.safetensors"), adapter_params)
            else:
                full_params = dict(tree_flatten(model.parameters()))
                mx.save_safetensors(str(temp_path / "model.safetensors"), full_params)

            if metadata.get("save_optimizer_state", False) and optimizer:
                mx.save_safetensors(str(temp_path / "optimizer.safetensors"), dict(tree_flatten(optimizer.state)))

            metadata["current_metric"] = current_metric
            with open(temp_path / "metadata.json", "w") as f: json.dump(metadata, f, indent=2, default=str)

            os.rename(temp_path, final_path)
            self._checkpoints.append(final_path)
            rprint(f"Checkpoint saved to [cyan]{final_path.name}[/cyan] (Metric: {current_metric:.4f}).")

            self._update_symlink(final_path, "latest")
            if self.is_best_metric(current_metric):
                self.best_metric = current_metric
                self._update_symlink(final_path, "best")
            
            self._rotate_checkpoints()
        except Exception as e:
            shutil.rmtree(temp_path, ignore_errors=True)
            raise CheckpointError(f"Atomic save failed for step {step}: {e}") from e

    def load_latest_state(self, model: nn.Module, optimizer: optim.Optimizer) -> Tuple[int, Dict[str, Any]]:
        chosen_path = self.resume_from_path
        if not chosen_path:
            latest_symlink = self.output_dir / "latest"
            if latest_symlink.is_symlink() and latest_symlink.resolve().is_dir():
                chosen_path = latest_symlink.resolve()
            elif self._checkpoints:
                chosen_path = self._checkpoints[-1]
        
        if not chosen_path or not chosen_path.exists():
            rprint("[yellow]No checkpoint found. Starting from scratch.[/yellow]")
            return 0, {}

        try:
            with open(chosen_path / "metadata.json", "r") as f: metadata = json.load(f)

            if (chosen_path / "model.safetensors").exists():
                model.load_weights(list(mx.load(str(chosen_path / "model.safetensors")).items()))
            
            if any(isinstance(m, MLXLoRALinear) for _, m in model.named_modules()) and (chosen_path / "adapters.safetensors").exists():
                from mlx_lm.tuner.utils import load_adapters
                load_adapters(model, str(chosen_path))

            if metadata.get("save_optimizer_state") and (chosen_path / "optimizer.safetensors").exists():
                optimizer.state = tree_unflatten(list(mx.load(str(chosen_path / "optimizer.safetensors")).items()))

            mx.eval(model.parameters(), optimizer.state)
            self.best_metric = metadata.get("current_metric", -float("inf"))
            resumed_updates = metadata.get("num_updates", 0)
            return resumed_updates, metadata
        except Exception as e:
            raise CheckpointError(f"Failed to load state from {chosen_path.name}: {e}") from e

    def _update_symlink(self, target_path: Path, link_name: str):
        link_path = self.output_dir / link_name
        if link_path.is_symlink() or link_path.exists(): link_path.unlink()
        os.symlink(os.path.relpath(target_path, self.output_dir), link_path, target_is_directory=True)

    def _rotate_checkpoints(self):
        self._checkpoints = sorted([p for p in self.output_dir.glob("checkpoint_*") if p.is_dir()], key=lambda p: p.stat().st_mtime)
        best_path = (self.output_dir / "best").resolve() if (self.output_dir / "best").is_symlink() else None
        
        to_delete = [chk for chk in self._checkpoints[:-self.keep_last_n] if chk != best_path]
        for chk in to_delete:
            shutil.rmtree(chk, ignore_errors=True)
            if chk in self._checkpoints: self._checkpoints.remove(chk)

    def is_best_metric(self, current_metric: float) -> bool:
        return self.save_best and current_metric > self.best_metric
EOF

echo "--- Updating core/__init__.py to export exceptions ---"

# 4. Update core/__init__.py to export the new exceptions.
cat <<'EOF' > ./src/mlx_rl_trainer/core/__init__.py
"""Core abstractions that define the trainer's architecture."""

from .config import (
    ExperimentConfig,
    RewardConfig,
    EvaluatorConfig,
    DataConfig,
    ModelConfig,
    TrainerParams,
    GenerationConfig,
    CheckpointConfig,
    MonitoringConfig,
)
from .exceptions import (
    CustomBaseException,
    ModelLoadError,
    DataLoadError,
    CheckpointError,
    InvalidConfigurationError,
    TrainingRuntimeError,
)
from .trainer import (
    BaseTrainer,
    TrainingMetrics,
    EvaluationMetrics,
)
from .model_manager import ModelManager
from .dataset_manager import DatasetManager
from .checkpoint_manager import CheckpointManager

__all__ = [
    "ExperimentConfig", "RewardConfig", "EvaluatorConfig", "DataConfig", "ModelConfig", "TrainerParams",
    "GenerationConfig", "CheckpointConfig", "MonitoringConfig",
    "BaseTrainer", "TrainingMetrics", "EvaluationMetrics", "CustomBaseException", "ModelLoadError", 
    "InvalidConfigurationError", "DataLoadError", "CheckpointError", "TrainingRuntimeError",
    "ModelManager", "DatasetManager", "CheckpointManager",
]
EOF

echo "Circular import fixed. All files have been updated."
