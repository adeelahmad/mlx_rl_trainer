# file_path: mlx_rl_trainer/src/mlx_rl_trainer/rewards/context.py
# revision_no: 001
# goals_of_writing_code_block: Define a predictable Pydantic data structure to serve as the context for all reward computations.
# type_of_code_response: add new code
"""
Data context required for reward computations.
"""
from pydantic import BaseModel, Field, ConfigDict
from typing import Dict, Any, List, Optional


class RewardContext(BaseModel):
    """
    A predictable and immutable data structure passed to all reward functions.

    Attributes:
        generated_text: The full text generated by the model for the current rollout.
        prompt_text: The original input prompt that led to `generated_text`.
        reference_completion: The ground truth / reference answer for comparison.
        test_cases: A list of dictionaries, each representing an executable test case
                    for code generation tasks (e.g., `[{"input": "2,3", "expected": "5"}]`).
        metadata: A dictionary for any additional, arbitrary context relevant to
                  reward computation (e.g., MCQ options, task type flags).
    """

    model_config = ConfigDict(frozen=True)

    generated_text: str = Field(
        ..., description="The full generated text sequence from the model."
    )
    prompt_text: str = Field(
        ..., description="The original input prompt that generated the text."
    )
    reference_completion: str = Field(
        ..., description="The ground truth reference completion for comparison."
    )

    test_cases: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="A list of test case dictionaries for code evaluation rewards.",
    )
    metadata: Dict[str, Any] = Field(
        default_factory=dict, description="Additional arbitrary metadata for context."
    )
