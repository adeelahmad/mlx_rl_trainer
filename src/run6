#!/bin/bash
set -e

echo "Fixing MLX RL Trainer codebase issues..."

# Fix 1: metrics_logger.py - already has time import, just verify
echo "Checking metrics_logger.py..."

# Fix 2: Add missing numpy import in perplexity.py
echo "Fixing perplexity.py..."
if ! grep -q "import numpy as np" ./mlx_rl_trainer/evaluation/general/perplexity.py; then
    perl -i -pe 's/^(from typing import Dict, Any, List)$/import numpy as np\n$1/' ./mlx_rl_trainer/evaluation/general/perplexity.py
fi

# Fix 3: Fix evaluate.py imports
echo "Fixing evaluate.py..."
if ! grep -q "^import json" ./mlx_rl_trainer/scripts/evaluate.py; then
    perl -i -pe 's/^(import argparse)$/import json\n$1/' ./mlx_rl_trainer/scripts/evaluate.py
fi

# Fix 4: Fix data/dataset_manager.py imports
echo "Fixing data/dataset_manager.py imports..."
cat > ./mlx_rl_trainer/data/dataset_manager.py << 'EOF'
"""
Data pipeline management: Loading, preprocessing, and efficient batching.
"""
import asyncio, aiofiles, json, logging, random, re
from pathlib import Path
from typing import Any, Dict, List, Optional, Iterator, Tuple
import mlx.core as mx
from datasets import Dataset, load_dataset
from tqdm.auto import tqdm
from ..core.config import DataConfig, THINK_STYLE_PROMPT_LITERAL
from ..core.exceptions import DataLoadError, TrainingRuntimeError
from mlx_lm.tokenizer_utils import TokenizerWrapper
from mlx_rl_trainer.utils.text_utils import (
    _contains_keywords,
    _mcq_meta_from_sample,
    apply_chat_template_wrapper,
)

logger = logging.getLogger(__name__)


def _ascii_ratio(s: str) -> float:
    if not s:
        return 1.0
    return sum(1 for ch in s if 32 <= ord(ch) <= 126 or ch in "\n\r\t") / max(1, len(s))


def _looks_garbage(s: str) -> bool:
    if not s or len(s.strip()) < 3 or len(s) > 20000 or _ascii_ratio(s) < 0.75:
        return True
    bad = re.findall(r"[^\w\s\-\.\,\:\;\(\)\[\]\/\+\=\&\<\>]", s)
    return (len(bad) / max(1, len(s))) > 0.15


def _normalize_record(
    obj: Dict[str, Any], prompt_key: str, completion_key: str, system_prompt: str
) -> Optional[Dict[str, Any]]:
    if not isinstance(obj, dict):
        return None

    def _s(x: Any) -> str:
        return str(x) if x is not None else ""

    prompt = _s(obj.get(prompt_key, obj.get("prompt", obj.get("question", ""))))
    completion = _s(
        obj.get(completion_key, obj.get("completion", obj.get("response", "")))
    )

    test_cases = obj.get("test_cases", [])
    if not isinstance(test_cases, list):
        test_cases = [test_cases] if test_cases else []

    rec = {
        "prompt": prompt,
        "completion": completion,
        "system": _s(obj.get("system", system_prompt)),
        "is_invalid_sample": obj.get("is_invalid_sample", False),
        "test_cases": test_cases,
        "meta": obj.get("meta", {}),
    }
    if not rec["prompt"] and not rec["completion"]:
        return None

    mcq_meta = _mcq_meta_from_sample(
        {"prompt": rec["prompt"], "completion": rec["completion"], "meta": rec["meta"]}
    )
    rec["meta"].update(mcq_meta)
    return rec


class DatasetManager:
    """Manages loading, filtering, tokenizing, and batching of datasets."""

    def __init__(self, config: DataConfig, tokenizer: Optional[TokenizerWrapper]):
        self.config = config
        self._tokenizer = tokenizer
        self._train_dataset: Optional[Dataset] = None
        self._val_dataset: Optional[Dataset] = None
        self._is_loaded = False
        self.system_prompt: str = ""
        logger.debug("DatasetManager initialized.")

    def set_tokenizer(self, tokenizer: TokenizerWrapper):
        if not isinstance(tokenizer, TokenizerWrapper):
            raise ValueError(f"Expected TokenizerWrapper, got {type(tokenizer)}")
        self._tokenizer = tokenizer
        logger.debug(f"DatasetManager tokenizer updated.")

    def set_system_prompt(self, system_prompt: str):
        self.system_prompt = system_prompt
        logger.debug("DatasetManager system prompt set.")

    async def _async_read_jsonl(self, path: Path) -> List[Dict[str, Any]]:
        if not path.exists():
            raise FileNotFoundError(f"Data file not found: {path}")
        data = []
        async with aiofiles.open(path, mode="r", encoding="utf-8") as f:
            async for line in f:
                if line.strip():
                    try:
                        data.append(json.loads(line.strip()))
                    except json.JSONDecodeError:
                        logger.warning(
                            f"Malformed JSONL line in {path.name}, skipping."
                        )
        return data

    async def load_datasets(self, force_reload: bool = False):
        if self._is_loaded and not force_reload:
            return
        loop = asyncio.get_event_loop()

        async def load_raw_data_for_split(
            path: Path, split_name: str
        ) -> List[Dict[str, Any]]:
            if not path:
                return []
            if path.suffix.lower() in [".jsonl", ".ndjson"]:
                return await self._async_read_jsonl(path)
            elif path.suffix.lower() == ".json":
                content = await aiofiles.open(path, "r", encoding="utf-8").read()
                return json.loads(content)
            else:
                hf_split = getattr(
                    self.config, f"dataset_{split_name}_split", split_name
                )
                dataset_obj = await asyncio.to_thread(
                    load_dataset, path.as_posix(), split=hf_split
                )
                return (
                    dataset_obj.to_list()
                    if hasattr(dataset_obj, "to_list")
                    else list(dataset_obj)
                )

        raw_train_data = await load_raw_data_for_split(self.config.train_path, "train")
        raw_val_data = (
            await load_raw_data_for_split(self.config.val_path, "val")
            if self.config.val_path
            else []
        )

        self._train_dataset = self._process_raw_to_dataset(raw_train_data, "train")
        self._val_dataset = (
            self._process_raw_to_dataset(raw_val_data, "val") if raw_val_data else None
        )

        self._is_loaded = True
        logger.info(
            f"Datasets loaded. Train samples: {len(self._train_dataset)}, Val samples: {len(self._val_dataset) if self._val_dataset else 0}"
        )

    def _process_raw_to_dataset(
        self, raw_data: List[Dict[str, Any]], split_name: str
    ) -> Dataset:
        normalized_records = []
        for obj in tqdm(raw_data, desc=f"Normalizing {split_name} data"):
            rec = _normalize_record(
                obj,
                self.config.dataset_prompt_key,
                self.config.dataset_answer_key,
                self.system_prompt,
            )
            if (
                rec
                and not _looks_garbage(rec["prompt"])
                and not (
                    _contains_keywords(
                        rec["prompt"], self.config.dataset_filter_keywords
                    )
                    or _contains_keywords(
                        rec["completion"], self.config.dataset_filter_keywords
                    )
                )
            ):
                normalized_records.append(rec)

        if not normalized_records:
            logger.warning(
                f"No valid records found for {split_name} after normalization and filtering."
            )
            return Dataset.from_list([])

        return Dataset.from_list(normalized_records)

    @property
    def tokenizer(self):
        return self._tokenizer

    def get_dataloader(self, split: str, batch_size: int) -> Iterator[Dict[str, Any]]:
        dataset = self._train_dataset if split == "train" else self._val_dataset
        if not dataset or len(dataset) == 0:
            return iter([])

        indices = list(range(len(dataset)))
        if self.config.shuffle_data and split == "train":
            random.shuffle(indices)

        def batch_generator():
            for i in range(0, len(indices), batch_size):
                batch_indices = indices[i : i + batch_size]
                raw_batch_list = dataset.select(batch_indices).to_list()
                if raw_batch_list:
                    try:
                        from mlx_rl_trainer.data.batch_builder import build_rollout_batch
                        prompts_data, prompts_mx, _ = build_rollout_batch(
                            self._tokenizer, dataset, batch_indices, self.config
                        )
                        if prompts_mx.size > 0:
                            yield {"prompts_data": prompts_data}
                    except Exception as e:
                        logger.error(
                            f"Error preparing batch {i//batch_size}: {e}. Skipping batch.",
                            exc_info=True,
                        )

        return batch_generator()
EOF

# Fix 5: Fix core/dataset_manager.py imports
echo "Fixing core/dataset_manager.py..."
perl -i -pe 's/from \.\.core\.trainer import DataLoadError, TrainingRuntimeError/from mlx_rl_trainer.core.exceptions import DataLoadError, TrainingRuntimeError/' ./mlx_rl_trainer/core/dataset_manager.py 2>/dev/null || true

# Fix 6: Fix __init__.py to import from correct location
echo "Fixing core/__init__.py..."
cat > ./mlx_rl_trainer/core/__init__.py << 'EOF'
"""Core abstractions that define the trainer's architecture."""

from .config import (
    ExperimentConfig,
    RewardConfig,
    EvaluatorConfig,
    DataConfig,
    ModelConfig,
    TrainerParams,
    GenerationConfig,
    CheckpointConfig,
    MonitoringConfig,
)
from .exceptions import (
    CustomBaseException,
    ModelLoadError,
    DataLoadError,
    CheckpointError,
    InvalidConfigurationError,
    TrainingRuntimeError,
)
from .trainer import (
    BaseTrainer,
    TrainingMetrics,
    EvaluationMetrics,
)
from .model_manager import ModelManager
from .checkpoint_manager import CheckpointManager

__all__ = [
    "ExperimentConfig", "RewardConfig", "EvaluatorConfig", "DataConfig", "ModelConfig", "TrainerParams",
    "GenerationConfig", "CheckpointConfig", "MonitoringConfig",
    "BaseTrainer", "TrainingMetrics", "EvaluationMetrics", "CustomBaseException", "ModelLoadError", 
    "InvalidConfigurationError", "DataLoadError", "CheckpointError", "TrainingRuntimeError",
    "ModelManager", "CheckpointManager",
]
EOF

# Fix 7: Fix main __init__.py
echo "Fixing main __init__.py..."
cat > ./mlx_rl_trainer/__init__.py << 'EOF'
"""
MLX RL Trainer - Production-ready reinforcement learning framework
"""
__version__ = "0.1.0"

from mlx_rl_trainer.core.trainer import BaseTrainer, TrainingMetrics, EvaluationMetrics
from mlx_rl_trainer.core.exceptions import (
    CustomBaseException,
    ModelLoadError,
    DataLoadError,
    CheckpointError,
    InvalidConfigurationError,
    TrainingRuntimeError,
)
from mlx_rl_trainer.core.config import (
    ExperimentConfig,
    RewardConfig,
    EvaluatorConfig,
    DataConfig,
    ModelConfig,
    TrainerParams,
    GenerationConfig,
    CheckpointConfig,
    MonitoringConfig,
)
from mlx_rl_trainer.core.model_manager import ModelManager
from mlx_rl_trainer.core.checkpoint_manager import CheckpointManager
from mlx_rl_trainer.rewards.registry import RewardRegistry, register_reward
from mlx_rl_trainer.rewards.base_reward import RewardComposer
from mlx_rl_trainer.rewards.context import RewardContext
from mlx_rl_trainer.evaluation.registry import EvaluatorRegistry

__all__ = [
    "BaseTrainer",
    "TrainingMetrics",
    "EvaluationMetrics",
    "CustomBaseException",
    "ModelLoadError",
    "DataLoadError",
    "CheckpointError",
    "InvalidConfigurationError",
    "TrainingRuntimeError",
    "ExperimentConfig",
    "RewardConfig",
    "EvaluatorConfig",
    "DataConfig",
    "ModelConfig",
    "TrainerParams",
    "GenerationConfig",
    "CheckpointConfig",
    "MonitoringConfig",
    "ModelManager",
    "CheckpointManager",
    "RewardRegistry",
    "register_reward",
    "RewardComposer",
    "RewardContext",
    "EvaluatorRegistry",
]
EOF

# Fix 8: Fix data batch_builder.py to use correct config import
echo "Fixing batch_builder.py..."
perl -i -pe 's/from mlx_rl_trainer\.core\.config import ExperimentConfig/from mlx_rl_trainer.core.config import DataConfig, ExperimentConfig/' ./mlx_rl_trainer/data/batch_builder.py 2>/dev/null || true

# Fix 9: Update batch_builder to accept DataConfig
cat > ./mlx_rl_trainer/data/batch_builder.py << 'EOF'
import logging
import json
from typing import Dict, Any, List, Tuple, Optional
from datasets import Dataset
import mlx.core as mx

from mlx_rl_trainer.core.config import DataConfig
from mlx_rl_trainer.utils.text_utils import (
    _mcq_meta_from_sample,
    apply_chat_template_wrapper,
)
from mlx_lm.tokenizer_utils import TokenizerWrapper
from mlx_rl_trainer.rewards.format.tag_structure import (
    extract_think_region,
    extract_answer_region,
)
from mlx_rl_trainer.core.config import GenerationConfig


logger = logging.getLogger(__name__)


def _compose_prompt_from_sample(
    sample: Dict[str, Any]
) -> Tuple[str, Optional[str], Optional[str]]:
    ref_ans, ref_think = None, None

    if "prompt" in sample and isinstance(sample["prompt"], str):
        prompt_text = sample["prompt"]
    elif "question" in sample and isinstance(sample["question"], str):
        prompt_text = sample["question"]
    else:
        prompt_text = json.dumps(sample, ensure_ascii=False)

    completion = sample.get("completion", sample.get("answer", ""))
    if isinstance(completion, str):
        gen_config = GenerationConfig()
        ref_think = extract_think_region(completion, gen_config)
        ref_ans = extract_answer_region(completion, gen_config) or completion.strip()

    return prompt_text, ref_ans, ref_think


def build_rollout_batch(
    tokenizer: TokenizerWrapper,
    dataset: Dataset,
    indices: List[int],
    config: DataConfig,
) -> Tuple[List[Dict[str, Any]], mx.array, int]:
    prompts_data: List[Dict[str, Any]] = []
    max_len_in_batch = 0
    pad_id = tokenizer.pad_token_id
    system_prompt = getattr(config, 'system_prompt', '')

    for i in indices:
        try:
            raw = dataset[i]
            prompt_text, ref_ans, ref_think = _compose_prompt_from_sample(raw)

            mcq_meta = _mcq_meta_from_sample(
                {
                    "prompt": prompt_text,
                    "completion": ref_ans,
                    "meta": raw.get("meta", {}),
                }
            )

            formatted_prompt = apply_chat_template_wrapper(
                tokenizer, prompt_text, system_prompt
            )
            p_tokens = tokenizer.encode(formatted_prompt, add_special_tokens=False)

            if len(p_tokens) > config.max_prompt_len:
                p_tokens = p_tokens[-config.max_prompt_len :]
            if not p_tokens:
                logger.warning(f"Skipping empty prompt (idx {i}).")
                continue

            entry = {
                "original_index": i,
                "text": formatted_prompt,
                "tokens": p_tokens,
                "ref_answer_str": ref_ans,
                "ref_think_str": ref_think,
                "is_invalid_sample": raw.get("is_invalid_sample", False),
            }
            entry.update(mcq_meta)
            prompts_data.append(entry)
            max_len_in_batch = max(max_len_in_batch, len(p_tokens))

        except Exception as e:
            logger.warning(f"Skipping sample idx {i} due to error: {e}")

    if not prompts_data:
        return [], mx.array([], dtype=mx.int32), 0

    padded_tokens = []
    for p in prompts_data:
        tok = p["tokens"]
        pad_len = max_len_in_batch - len(tok)
        padded_tokens.append([pad_id] * pad_len + tok)

    return prompts_data, mx.array(padded_tokens, dtype=mx.int32), max_len_in_batch
EOF

echo ""
echo "All fixes applied successfully!"
echo ""
echo "Summary of fixes:"
echo "1. Fixed perplexity.py numpy import"
echo "2. Fixed evaluate.py json import"
echo "3. Fixed data/dataset_manager.py to import from core.exceptions"
echo "4. Fixed core/__init__.py imports"
echo "5. Fixed main __init__.py to import from correct locations"
echo "6. Fixed batch_builder.py to handle DataConfig properly"
echo ""
echo "Please try running mlx-train again."
