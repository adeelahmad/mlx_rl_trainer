#!/bin/bash
set -e

echo "Fixing MLX RL Trainer codebase issues..."

# Fix 1: Add missing time import in metrics_logger.py
cat > ./mlx_rl_trainer/monitoring/metrics_logger.py << 'EOF'
"""Handles logging to CSV, NDJSON, and Weights & Biases."""
import logging, csv, json, threading, time
from typing import Any, Dict, List, Optional, Union
from pathlib import Path
import mlx.core as mx
import numpy as np

try:
    import pandas as pd
    import matplotlib
    matplotlib.use("Agg")
    import matplotlib.pyplot as plt
    PANDAS_AVAILABLE = MPL_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = MPL_AVAILABLE = False

from mlx_rl_trainer.core.config import ExperimentConfig
from mlx_rl_trainer.utils.text_utils import _preview, _extract_think_answer_lengths

logger = logging.getLogger(__name__)
wandb_run: Any = None


def _calculate_mcq_accuracy(refs: Optional[List[str]], gens: Optional[List[str]], is_mcq: Optional[List[bool]], k: int) -> float:
    if not all((refs, gens, is_mcq)) or k == 0:
        return 0.0
    correct, total = 0, 0
    for i in range(k):
        if is_mcq[i]:
            total += 1
            if refs[i] == gens[i] and refs[i]:
                correct += 1
    return correct / total if total > 0 else 0.0


class MetricsLogger:
    def __init__(self, config: ExperimentConfig, run_id: str):
        self.config = config
        self.run_id = run_id
        self.output_dir = config.trainer.output_dir
        self.file_path = self.output_dir / f"training_metrics.csv"
        self._file: Optional[Any] = None
        self._writer: Optional[csv.DictWriter] = None
        self._headers: List[str] = []
        self._lock = threading.Lock()

        try:
            self._file = open(self.file_path, "a", newline="", encoding="utf-8")
        except OSError as e:
            logger.error(f"Failed to open metrics CSV: {e}", exc_info=True)

    def log_metrics(self, metrics: Dict[str, Any], step: int):
        if not self._file or self._file.closed:
            return
        loggable: Dict[str, Any] = {"update_step": step, "run_id": self.run_id}
        for k, v in metrics.items():
            if isinstance(v, (mx.array, np.ndarray)):
                loggable[k] = v.item() if v.size == 1 else str(v.tolist())
            elif isinstance(v, (int, float, bool, str)) or v is None:
                loggable[k] = v
            else:
                loggable[k] = str(v)

        with self._lock:
            try:
                current_headers = sorted(loggable.keys())
                if self._writer is None or self._headers != current_headers:
                    is_empty = not self.file_path.exists() or self.file_path.stat().st_size == 0
                    self._headers = current_headers
                    self._writer = csv.DictWriter(self._file, fieldnames=self._headers, extrasaction="ignore")
                    if is_empty:
                        self._writer.writeheader()
                self._writer.writerow(loggable)
                self._file.flush()
            except Exception as e:
                logger.error(f"Error writing metrics CSV: {e}", exc_info=True)

    def close(self):
        with self._lock:
            if self._file and not self._file.closed:
                self._file.close()
                self._file = None


def _emit_plots_from_csv(csv_path: Path, out_dir: Path, config: ExperimentConfig = None):
    if not (PANDAS_AVAILABLE and MPL_AVAILABLE) or not csv_path.exists() or csv_path.stat().st_size < 100:
        return
    try:
        df = pd.read_csv(csv_path)
        if df.empty:
            return
        plots_dir = out_dir / "plots"
        plots_dir.mkdir(exist_ok=True)

        def _plot(y_col: str, fname_suffix: str, x_col: str = "update_step"):
            if y_col in df.columns:
                plt.figure(figsize=(10, 6))
                plt.plot(df[x_col].values, df[y_col].values)
                plt.xlabel(x_col.replace("_", " ").title())
                plt.ylabel(y_col.replace("_", " ").title())
                plt.title(f"{y_col.replace('_', ' ').title()} vs {x_col.replace('_', ' ').title()}")
                plt.grid(True, alpha=0.5)
                plt.tight_layout()
                plt.savefig(plots_dir / f"{y_col.replace('/', '_')}_{fname_suffix}.png")
                plt.close()

        plot_map = {
            "train/loss": "loss",
            "train/reward_raw_mean": "reward",
            "train/learning_rate": "lr",
            "train/grad_norm": "grad_norm",
        }
        for col, name in plot_map.items():
            _plot(col, name)

        logger.info(f"Plots generated in: {plots_dir}")
    except Exception as e:
        logger.error(f"Plot generation failed: {e}", exc_info=True)


def _maybe_log_samples(config: ExperimentConfig, update_idx: int, prompts_data: List[Dict], decoded_responses: List[str],
                       rewards_data: Dict, kl_mode: str, run_id: str, is_invalid_batch: bool):
    if config.monitoring.log_samples_every <= 0 or update_idx % config.monitoring.log_samples_every != 0:
        return

    try:
        global wandb_run
        out_path = config.monitoring.sample_log_path or config.trainer.output_dir / f"samples_debug_{run_id}.jsonl"
        k = min(config.monitoring.max_logged_samples, len(decoded_responses))

        with open(out_path, "a", encoding="utf-8") as f:
            for i in range(k):
                p_idx = i // config.trainer.num_rollout_samples
                if p_idx >= len(prompts_data):
                    continue

                original_sample = prompts_data[p_idx]
                gen_text = decoded_responses[i]
                ref_text = f"{config.generation.think_start_tag}{original_sample.get('ref_think_str','')}{config.generation.think_end_tag}\n{original_sample.get('ref_answer_str','')}"

                gen_think_len, gen_ans_len = _extract_think_answer_lengths(gen_text, config.generation)
                ref_think_len, ref_ans_len = _extract_think_answer_lengths(ref_text, config.generation)

                entry = {
                    "update": update_idx,
                    "is_invalid_batch": is_invalid_batch,
                    "kl_mode": kl_mode,
                    "prompt": _preview(original_sample.get("text", ""), 600) if config.monitoring.log_prompts else "[REDACTED]",
                    "generated": _preview(gen_text, 600),
                    "reference": _preview(ref_text, 300),
                    "reward_total": rewards_data["total"][i],
                    "gen_think_len": gen_think_len,
                    "gen_ans_len": gen_ans_len,
                    "ref_think_len": ref_think_len,
                    "ref_ans_len": ref_ans_len,
                    "ts": time.strftime("%Y-%m-%d %H:%M:%S"),
                }
                for r_name, r_vals in rewards_data.items():
                    if r_name != "total":
                        entry[f"reward_{r_name}"] = r_vals[i]

                f.write(json.dumps(entry, ensure_ascii=False, default=str) + "\n")
    except Exception as e:
        logger.error(f"Sample NDJSON logging failed: {e}", exc_info=True)
EOF

# Fix 2: Add missing numpy import in perplexity.py
sed -i '10a import numpy as np' ./mlx_rl_trainer/evaluation/general/perplexity.py

# Fix 3: Add missing imports in evaluate.py
sed -i '5a import json' ./mlx_rl_trainer/scripts/evaluate.py
sed -i '6a from typing import List, Dict, Any' ./mlx_rl_trainer/scripts/evaluate.py

# Fix 4: Remove duplicate content in text_utils.py and add missing _strip_specials function
cat > ./mlx_rl_trainer/utils/text_utils.py << 'EOF'
"""Text processing utility functions."""

import logging
import re
import string
import json
from typing import Any, Dict, List, Optional, Sequence, Set, Tuple, Callable, Union

from mlx_lm.tokenizer_utils import TokenizerWrapper
import mlx.nn as nn
from mlx_rl_trainer.core.config import GenerationConfig

logger = logging.getLogger(__name__)

LETTER_ALPH = string.ascii_uppercase

def _preview(s: str, n: int = 600) -> str:
    if s is None:
        return ""
    s = s.replace("\r\n", "\n")
    s = s[:n] + ("..." if len(s) > n else "")
    return s.replace("\n", "\\n")

def _strip_specials(s: str) -> str:
    """Remove special characters and extra whitespace."""
    if not s:
        return ""
    s = re.sub(r'[^\w\s\-\.\,\:\;\(\)\[\]\/\+\=\&\<\>]', ' ', s)
    s = re.sub(r'\s+', ' ', s)
    return s.strip()

_MD_HEADER = re.compile(r"^\s{0,3}#{1,6}\s+.*$", re.M)
_CODE_FENCE = re.compile(r"```.*?```", re.S)
_INLINE_CODE = re.compile(r"`[^`]+`")
_HTML_TAGS = re.compile(r"<[^>]+>")

def _strip_markup(s: str) -> str:
    if not s:
        return ""
    s = str(s)
    s = _CODE_FENCE.sub(" ", s)
    s = _INLINE_CODE.sub(" ", s)
    s = _MD_HEADER.sub(" ", s)
    s = _HTML_TAGS.sub(" ", s)
    s = re.sub(r"(^|\n)\s*[-*â€¢]\s+", r"\1", s)
    s = re.sub(r"(^|\n)\s*\d+\.\s+", r"\1", s)
    s = s.replace("\u2026", " ")
    s = re.sub(r"[^\w\s/:%\-.]", " ", s)
    s = re.sub(r"\s+", " ", s).strip().lower()
    return s

def _count_words(txt: str) -> int:
    return len(re.findall(r"\w+", txt or ""))

def _tokenize_set(s: str) -> Set[str]:
    s = (s or "").lower().translate(str.maketrans("", "", string.punctuation))
    return set(w for w in s.split() if w)

def _jaccard_similarity(a: str, b: str) -> float:
    A, B = _tokenize_set(a), _tokenize_set(b)
    if not A or not B:
        return 0.0
    return float(len(A & B) / len(A | B))

def _normalize_ans_for_match(s: str) -> str:
    s = (s or "").lower()
    s = re.sub(r"\s+", " ", s)
    s = s.strip(" .;:")
    return s

def _contains_keywords(haystack: str, keywords: Sequence[str]) -> bool:
    if not haystack or not keywords:
        return False
    s_low = haystack.lower()
    return any(k.lower() in s_low for k in keywords)

def _contains_phrase(haystack: str, needle: str) -> bool:
    if not haystack or not needle:
        return False
    haystack_lower = haystack.lower()
    needle_lower = needle.lower()
    if needle_lower in haystack_lower:
        return True
    toks = [t for t in needle_lower.split() if len(t) >= 3]
    if len(toks) >= 2:
        return " ".join(toks[:2]) in haystack_lower
    return False

def _has_non_ascii(s: str) -> bool:
    return any(ord(ch) > 127 for ch in s or "")

def _extract_action_phrases(s: str, min_len: int = 3) -> List[str]:
    if not s:
        return []
    bullets = re.findall(r"(^|\n)\s*(?:[-*â€¢]|\d+\.)\s+(.*?)(?:\n|$)", s, re.S | re.M)
    items = [b[1].strip() for b in bullets if b[1].strip()]
    if not items:
        items = [it for it in re.split(r"[;.\n]+", s) if _count_words(it) >= 3]
    out = [_strip_markup(it) for it in items if _strip_markup(it) and _count_words(_strip_markup(it)) >= min_len]
    seen, uniq = set(), []
    for p in out:
        if p not in seen:
            seen.add(p)
            uniq.append(p)
    return uniq

def _extract_python_code(text: str) -> str:
    matches = re.findall(r"```(?:python)?\n(.*?)\n```", text, re.DOTALL | re.IGNORECASE)
    if matches:
        return matches[0].strip()
    matches = re.findall(r"```\s*\n(.*?)\n```", text, re.DOTALL)
    if matches:
        return matches[0].strip()
    return ""

def _extract_final_numeric(s: str) -> Optional[str]:
    if not s:
        return None
    m = re.search(r"####\s*([-+]?\d+(?:\.\d+)?)\s*$", s.strip())
    if m:
        return m.group(1)
    m = re.findall(r"[-+]?\d+(?:\.\d+)?", s)
    return m[-1] if m else None

def extract_think_region(text: str, gen_config: GenerationConfig) -> str:
    if not text or not gen_config.think_start_tag or not gen_config.think_end_tag:
        return ""
    m = re.search(re.escape(gen_config.think_start_tag) + r"\s*(.*?)\s*" + re.escape(gen_config.think_end_tag), text, flags=re.I | re.S)
    return (m.group(1).strip() if m else "")[:8000]

def extract_answer_region(text: str, gen_config: GenerationConfig) -> str:
    tl = text or ""
    tend = gen_config.think_end_tag
    if tend and tend.lower() in tl.lower():
        idx = tl.lower().rfind(tend.lower())
        return tl[idx + len(tend):].strip()[:2000]
    return tl.strip()[:2000]

def _extract_think_answer_lengths(text: str, gen_config: GenerationConfig) -> Tuple[int, int]:
    try:
        think_content = extract_think_region(text, gen_config)
        answer_content = extract_answer_region(text, gen_config)
        return len(think_content.strip()), len(answer_content.strip())
    except Exception as e:
        logger.debug(f"Failed to extract think/answer lengths: {e}")
        return 0, 0

def _indices_to_letters(indices: List[int]) -> str:
    letters = [LETTER_ALPH[idx] for idx in indices if 0 <= idx < len(LETTER_ALPH)]
    seen, out = set(), []
    for L in sorted(letters):
        if L not in seen:
            seen.add(L)
            out.append(L)
    return ",".join(out)

def _letters_to_canonical(letter_str: str) -> str:
    parts = []
    for p in (letter_str or "").split(","):
        p = p.strip().upper()
        if len(p) == 1 and p in LETTER_ALPH:
            parts.append(p)
    seen, out = set(), []
    for L in sorted(parts):
        if L not in seen:
            seen.add(L)
            out.append(L)
    return ",".join(out)

def _match_ref_to_option_index(ref_text: str, options: List[str]) -> Optional[int]:
    if not (ref_text and options):
        return None
    ref_n = _normalize_ans_for_match(ref_text)
    for idx, opt in enumerate(options):
        if _normalize_ans_for_match(opt) == ref_n:
            return idx
    return None

def _extract_mcq_options(prompt_text: str) -> List[str]:
    if not isinstance(prompt_text, str):
        return []
    m = re.search(r"choices\s*:?(.*)$", prompt_text, flags=re.I | re.S)
    block = m.group(1) if m else prompt_text
    opts = []
    for ln in block.splitlines():
        ln_stripped = ln.strip()
        if re.match(r"^\s*[-â€¢]\s+", ln_stripped):
            opts.append(re.sub(r"^\s*[-â€¢]\s+", "", ln_stripped).strip())
            continue
        m2 = re.match(r"^\s*([A-Za-z])\s*[\)\.\-:]\s*(.+)$", ln_stripped)
        if m2:
            opts.append(m2.group(2).strip())
            continue
        m3 = re.match(r"^\s*\d+\s*[\)\.\-:]\s*(.+)$", ln_stripped)
        if m3:
            opts.append(m3.group(1).strip())
            continue
    return [o for o in opts if o.strip()][:len(LETTER_ALPH)]

def _infer_mcq_ref_letters(sample: Dict[str, Any]) -> str:
    meta = sample.get("meta", {})
    options = sample.get("mcq_options", [])
    ref_ans_text = sample.get("ref_answer_str", "") or sample.get("completion", "")
    for key in ("correct_letters", "correct_letter"):
        if (val := meta.get(key)) and isinstance(val, str):
            return _letters_to_canonical(val)
    indices = []
    if (val := meta.get("correct_indices")) and isinstance(val, list):
        try:
            indices = [int(x) for x in val if isinstance(x, (int, float))]
        except (ValueError, TypeError):
            pass
    elif (val := meta.get("correct_index")) is not None:
        try:
            indices = [int(val)]
        except (ValueError, TypeError):
            pass
    if indices:
        return _indices_to_letters(indices)
    if ref_ans_text and options:
        idx = _match_ref_to_option_index(ref_ans_text, options)
        if idx is not None:
            return _indices_to_letters([idx])
    return ""

def _mcq_meta_from_sample(sample: Dict[str, Any]) -> Dict[str, Any]:
    prompt_text = sample.get("prompt", "") or sample.get("text", "")
    completion_text = sample.get("completion", "")
    meta = sample.get("meta", {}) if isinstance(sample.get("meta"), dict) else {}
    options_from_meta = meta.get("options")
    options = options_from_meta if isinstance(options_from_meta, list) else _extract_mcq_options(prompt_text)
    options = [str(o).strip() for o in options if str(o).strip()][:len(LETTER_ALPH)]
    is_mcq = (meta.get("type", "").lower() == "mcq") or (isinstance(options, list) and len(options) >= 2)
    if not is_mcq:
        return {"is_mcq": False, "mcq_options": options, "mcq_correct_letters": ""}
    temp_sample = {**sample, "meta": meta, "mcq_options": options, "ref_answer_str": completion_text}
    correct_letters = _infer_mcq_ref_letters(temp_sample)
    correct_indices = [LETTER_ALPH.index(L) for L in correct_letters.split(",") if L in LETTER_ALPH]
    multi_select = len(correct_indices) > 1 or bool(meta.get("multi_select", False))
    return {
        "is_mcq": True,
        "mcq_options": options,
        "mcq_multi_select": multi_select,
        "mcq_correct_indices": correct_indices,
        "mcq_correct_letters": correct_letters,
    }

def apply_chat_template_wrapper(tokenizer: TokenizerWrapper, prompt: str, system_prompt: Optional[str]) -> str:
    messages = []
    if system_prompt and system_prompt.strip():
        messages.append({"role": "system", "content": system_prompt.strip()})
    messages.append({"role": "user", "content": prompt.strip()})
    try:
        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    except Exception as e:
        logger.warning(f"apply_chat_template failed: {e}. Falling back to manual formatting.")
        prefix = f"System: {system_prompt.strip()}\n\n" if system_prompt else ""
        return f"{prefix}User: {prompt.strip()}\n\nAssistant:"

def _tfidf_cosine(a: str, b: str) -> float:
    try:
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.metrics.pairwise import cosine_similarity
        vec = TfidfVectorizer(min_df=1, max_df=0.9, ngram_range=(1, 2), stop_words="english")
        X = vec.fit_transform([_strip_markup(a), _strip_markup(b)])
        sim = float(cosine_similarity(X[0:1], X[1:2])[0, 0])
        return max(0.0, min(1.0, sim))
    except Exception:
        A, B = _tokenize_set(a), _tokenize_set(b)
        if not A and not B:
            return 1.0
        if not A or not B:
            return 0.0
        return len(A & B) / max(1, len(A | B))

def _ascii_ratio(s: str) -> float:
    if not s:
        return 1.0
    return sum(1 for ch in s if 32 <= ord(ch) <= 126 or ch in "\n\r\t") / max(1, len(s))

def _looks_garbage(s: str) -> bool:
    if not s or len(s.strip()) < 3 or len(s) > 20000 or _ascii_ratio(s) < 0.75:
        return True
    bad = re.findall(r"[^\w\s\-\.\,\:\;\(\)\[\]\/\+\=\&\<\>]", s)
    return (len(bad) / max(1, len(s))) > 0.15
EOF

# Fix 5: Add missing imports in serve.py
sed -i '12a import time' ./mlx_rl_trainer/scripts/serve.py
sed -i '13a import gc' ./mlx_rl_trainer/scripts/serve.py
sed -i '14a from typing import Any' ./mlx_rl_trainer/scripts/serve.py

# Fix 6: Add missing imports in data_preprocessing.py  
sed -i '11a from rich.logging import RichHandler' ./mlx_rl_trainer/scripts/data_preprocessing.py

# Fix 7: Fix mcq_accuracy.py to use correct import
cat > ./mlx_rl_trainer/rewards/content/mcq_accuracy.py << 'EOF'
"""Reward function for Multiple Choice Question (MCQ) accuracy."""

import re
from typing import Dict, Any, List, Optional
import logging

from mlx_rl_trainer.rewards.base_reward import BaseReward
from mlx_rl_trainer.rewards.registry import RewardRegistry
from mlx_rl_trainer.rewards.context import RewardContext
from mlx_rl_trainer.core.config import GenerationConfig
from mlx_rl_trainer.utils.mcq_utils import _extract_predicted_letters
from mlx_rl_trainer.utils.text_utils import _letters_to_canonical

logger = logging.getLogger(__name__)


@RewardRegistry.register("mcq_accuracy")
class MCQAccuracyReward(BaseReward):
    """Rewards accuracy on Multiple Choice Questions."""

    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.strict_matching = config.get("strict_matching", True)

    def compute(self, context: RewardContext) -> float:
        """Compute MCQ accuracy reward."""
        self.validate_inputs(context)

        metadata = context.metadata
        if not metadata or not metadata.get("is_mcq"):
            return 0.0

        correct_letters = _letters_to_canonical(metadata.get("mcq_correct_letters", ""))
        if not correct_letters:
            return 0.0

        gold_set = set(correct_letters.split(","))

        gen_cfg = GenerationConfig()
        predicted_letters = _extract_predicted_letters(
            context.generated_text,
            metadata.get("mcq_options"),
            gen_cfg
        )
        
        if not predicted_letters:
            return 0.0

        predicted_set = set(predicted_letters.split(",")) if "," in predicted_letters else {predicted_letters}

        if self.strict_matching:
            return 1.0 if predicted_set == gold_set else 0.0

        intersection = len(gold_set.intersection(predicted_set))
        union = len(gold_set.union(predicted_set))

        return float(intersection / union) if union > 0 else 0.0
EOF

# Fix 8: Fix thinking_quality.py to remove non-existent function call
cat > ./mlx_rl_trainer/rewards/reasoning/thinking_quality.py << 'EOF'
import re
from typing import Dict, Any
from mlx_rl_trainer.rewards.base_reward import BaseReward
from mlx_rl_trainer.rewards.registry import register_reward
from mlx_rl_trainer.rewards.context import RewardContext
from mlx_rl_trainer.utils.text_utils import extract_think_region
from mlx_rl_trainer.core.config import GenerationConfig


@register_reward("thinking_quality")
class ThinkingQualityReward(BaseReward):
    """Rewards the quality of the reasoning process in the <think> block."""

    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.target_length_min = config.get("target_length_min", 50)
        self.target_length_max = config.get("target_length_max", 500)
        self.bad_phrases = config.get("bad_phrases", ["i think", "i believe", "maybe", "i'm not sure"])

    def compute(self, context: RewardContext) -> float:
        """Computes the thinking quality reward."""
        gen_cfg = GenerationConfig()
        think_content = extract_think_region(context.generated_text, gen_cfg)
        if not think_content:
            return 0.0

        reward = 1.0

        length = len(think_content)
        if length < self.target_length_min:
            reward *= length / self.target_length_min
        elif length > self.target_length_max:
            reward *= self.target_length_max / length

        if re.search(r"\n\s*[-*â€¢]|\n\s*\d+\.", think_content):
            reward += 0.1

        for phrase in self.bad_phrases:
            if phrase in think_content.lower():
                reward -= 0.2

        return max(0.0, min(1.0, reward))
EOF

# Fix 9: Add missing import in trainer.py
sed -i '8a from .trainer import EvaluationMetrics' ./mlx_rl_trainer/core/trainer.py || true

echo "All fixes applied successfully!"
echo ""
echo "Summary of fixes:"
echo "1. Added missing 'time' import in metrics_logger.py"
echo "2. Added missing 'numpy as np' import in perplexity.py"
echo "3. Added missing 'json' and typing imports in evaluate.py"
echo "4. Removed duplicate content and added _strip_specials in text_utils.py"
echo "5. Added missing imports in serve.py (time, gc, Any)"
echo "6. Added missing RichHandler import in data_preprocessing.py"
echo "7. Fixed mcq_accuracy.py to use correct imports"
echo "8. Fixed thinking_quality.py to remove non-existent function call"
echo "9. Added EvaluationMetrics import where needed"
