#!/bin/bash
set -euo pipefail

echo "Applying fix for AttributeError 'max_prompt_len' in data/batch_builder.py."

# ══════════════════════════════════════════════════════════════════════════════
# 1. Update src/mlx_rl_trainer/data/batch_builder.py
#    - FIX: Correct the attribute access from `config.max_prompt_len` to
#      `config.data.max_prompt_len`.
# ══════════════════════════════════════════════════════════════════════════════
echo "Writing src/mlx_rl_trainer/data/batch_builder.py..."
cat <<'EOF' > ./src/mlx_rl_trainer/data/batch_builder.py
import logging
import json
from typing import Dict, Any, List, Tuple, Optional
from datasets import Dataset
import mlx.core as mx

from mlx_rl_trainer.core.config import ExperimentConfig, GenerationConfig
from mlx_rl_trainer.utils.text_utils import _mcq_meta_from_sample, apply_chat_template_wrapper, extract_think_region, extract_answer_region
from mlx_lm.tokenizer_utils import TokenizerWrapper

logger = logging.getLogger(__name__)

def _compose_prompt_from_sample(sample: Dict[str, Any]) -> Tuple[str, Optional[str], Optional[str]]:
    ref_ans, ref_think = None, None
    gen_config = GenerationConfig()

    prompt_text = sample.get('prompt', sample.get('question', ''))
    completion = sample.get('completion', sample.get('answer', ''))
    
    if isinstance(completion, str):
        ref_think = extract_think_region(completion, gen_config)
        ref_ans = extract_answer_region(completion, gen_config) or completion.strip()
    
    return prompt_text, ref_ans, ref_think

def build_rollout_batch(
    tokenizer: TokenizerWrapper,
    dataset: Dataset,
    indices: List[int],
    config: ExperimentConfig,
) -> Tuple[List[Dict[str, Any]], mx.array, int]:
    
    prompts_data: List[Dict[str, Any]] = []
    max_len_in_batch = 0
    pad_id = tokenizer.pad_token_id

    for i in indices:
        try:
            raw = dataset[i]
            prompt_text, ref_ans, ref_think = _compose_prompt_from_sample(raw)
            
            mcq_meta = _mcq_meta_from_sample({'prompt': prompt_text, 'completion': ref_ans, 'meta': raw.get('meta', {})})
            
            formatted_prompt = apply_chat_template_wrapper(tokenizer, prompt_text, config.system_prompt)
            p_tokens = tokenizer.encode(formatted_prompt, add_special_tokens=True)

            # CORRECTED: Access max_prompt_len via config.data
            if len(p_tokens) > config.data.max_prompt_len:
                p_tokens = p_tokens[-config.data.max_prompt_len:]
            if not p_tokens:
                logger.warning(f"Skipping empty prompt (idx {i}).")
                continue

            entry = {
                'original_index': i,
                'text': formatted_prompt,
                'tokens': p_tokens,
                'ref_answer_str': ref_ans,
                'ref_think_str': ref_think,
                'is_invalid_sample': raw.get('is_invalid_sample', False),
                'meta': raw.get('meta', {}) # Pass original meta along for reward context
            }
            entry['meta'].update(mcq_meta)
            prompts_data.append(entry)
            max_len_in_batch = max(max_len_in_batch, len(p_tokens))

        except Exception as e:
            logger.warning(f"Skipping sample idx {i} due to error during batch building: {e}", exc_info=True)

    if not prompts_data:
        return [], mx.array([], dtype=mx.int32), 0

    padded_tokens = []
    for p in prompts_data:
        tok = p['tokens']
        pad_len = max_len_in_batch - len(tok)
        padded_tokens.append([pad_id] * pad_len + tok)

    return prompts_data, mx.array(padded_tokens, dtype=mx.int32), max_len_in_batch
EOF

# ══════════════════════════════════════════════════════════════════════════════
# 2. Also correct the DataManager `get_dataloader` method which calls `build_rollout_batch`
# ══════════════════════════════════════════════════════════════════════════════
echo "Writing src/mlx_rl_trainer/core/dataset_manager.py..."
cat <<'EOF' > ./src/mlx_rl_trainer/core/dataset_manager.py
"""
Data pipeline management: Loading, preprocessing, and efficient batching.
"""
import json, logging, random, re, asyncio
from pathlib import Path
from typing import Any, Dict, List, Optional, Iterator
from datasets import Dataset, Features, Value, load_dataset
from tqdm.auto import tqdm
from mlx_rl_trainer.core.config import DataConfig, THINK_STYLE_PROMPT_LITERAL, GenerationConfig, ExperimentConfig
from mlx_rl_trainer.core.exceptions import DataLoadError
from mlx_lm.tokenizer_utils import TokenizerWrapper
from mlx_rl_trainer.utils.text_utils import (_contains_keywords, _mcq_meta_from_sample, apply_chat_template_wrapper, extract_think_region, _looks_garbage)
from mlx_rl_trainer.data.batch_builder import build_rollout_batch
import mlx.core as mx

logger = logging.getLogger(__name__)

def _normalize_record(obj: Dict[str, Any], prompt_key: str, completion_key: str, system_prompt_default: str) -> Optional[Dict[str, Any]]:
    if not isinstance(obj, dict): return None
    def _s(x: Any) -> str: return str(x) if x is not None else ""
    prompt = _s(obj.get(prompt_key, obj.get("prompt", obj.get("question", ""))))
    completion = _s(obj.get(completion_key, obj.get("completion", obj.get("answer", ""))))
    system = _s(obj.get("system", system_prompt_default))
    
    gen_config_default = GenerationConfig()
    completion_cleaned = (completion.replace(f'{gen_config_default.think_start_tag}\n{gen_config_default.think_start_tag}\n', gen_config_default.think_start_tag)
                                    .replace(f'{gen_config_default.think_end_tag}\n{gen_config_default.think_end_tag}', gen_config_default.think_end_tag)
                                    .replace(f'{gen_config_default.think_start_tag}\n\n{gen_config_default.think_start_tag}', gen_config_default.think_start_tag))
    if completion_cleaned and gen_config_default.think_start_tag not in completion_cleaned: 
        completion_cleaned = f"{gen_config_default.think_start_tag}\n\n{gen_config_default.think_end_tag}\n{completion_cleaned}"
    
    meta = obj.get('meta', {}) if isinstance(obj.get('meta'), dict) else {}
    mcq_meta = _mcq_meta_from_sample({"prompt": prompt, "completion": completion_cleaned, "meta": meta})
    
    final_meta = {
        'is_mcq': mcq_meta.get('is_mcq', False),
        'mcq_options': mcq_meta.get('mcq_options', []),
        'mcq_multi_select': mcq_meta.get('mcq_multi_select', False),
        'mcq_correct_indices': mcq_meta.get('mcq_correct_indices', []),
        'mcq_correct_letters': mcq_meta.get('mcq_correct_letters', ''),
    }
    final_meta.update({k: v for k, v in meta.items() if k not in final_meta})

    test_cases = obj.get('test_cases', [])
    if not isinstance(test_cases, list):
        test_cases = [test_cases] if test_cases is not None else []
    test_cases_str = [json.dumps(tc) if isinstance(tc, dict) else str(tc) for tc in test_cases]

    if not prompt.strip() and not completion_cleaned.strip() and not system.strip(): return None

    return {'prompt': prompt, 'completion': completion_cleaned, 'system': system, 'test_cases': test_cases_str, 'is_invalid_sample': obj.get('is_invalid_sample', False), 'meta': final_meta}

class DatasetManager:
    def __init__(self, config: DataConfig, tokenizer: Optional[TokenizerWrapper]):
        self.config = config
        self._tokenizer = tokenizer
        self._train_dataset: Optional[Dataset] = None
        self._val_dataset: Optional[Dataset] = None
        self._is_loaded = False
        self.system_prompt: str = ""
        logger.debug("DatasetManager initialized.")

    def set_tokenizer(self, tokenizer: TokenizerWrapper):
        self._tokenizer = tokenizer
    def set_system_prompt(self, system_prompt: str):
        self.system_prompt = system_prompt

    async def _async_read_jsonl(self, path: Path) -> List[Dict[str, Any]]:
        if not path.is_file(): raise FileNotFoundError(f"Data file not found: {path}")
        data = []
        async with aiofiles.open(path, mode="r", encoding="utf-8") as f:
            async for line in f:
                if line.strip():
                    try: data.append(json.loads(line.strip()))
                    except json.JSONDecodeError: logger.warning(f"Malformed JSONL line in {path.name}, skipping.")
        return data

    async def load_datasets(self, force_reload: bool = False):
        if self._is_loaded and not force_reload: return
        loop = asyncio.get_event_loop() 

        async def load_raw_data_for_split(path: Path, split_name: str) -> List[Dict[str, Any]]:
            if not path: return []
            if path.suffix.lower() in ['.jsonl', '.ndjson']: return await self._async_read_jsonl(path)
            elif path.suffix.lower() == '.json': 
                raw_content = await aiofiles.open(path, mode='r', encoding='utf-8').read()
                return json.loads(raw_content)
            else:
                hf_split_name = 'train' if split_name == 'train' else 'test'
                dataset_obj = await asyncio.to_thread(load_dataset, path.as_posix(), split=hf_split_name)
                return dataset_obj.to_list() if hasattr(dataset_obj, 'to_list') else list(dataset_obj)

        raw_train_data = await load_raw_data_for_split(self.config.train_path, "train")
        raw_val_data = await load_raw_data_for_split(self.config.val_path, "val") if self.config.val_path else []

        self._train_dataset = self._process_raw_to_dataset(raw_train_data, "train")
        self._val_dataset = self._process_raw_to_dataset(raw_val_data, "val") if raw_val_data else None
        self._is_loaded = True
        logger.info(f"Datasets loaded. Train: {len(self._train_dataset)}, Val: {len(self._val_dataset) if self._val_dataset else 0}")
    
    def _process_raw_to_dataset(self, raw_data: List[Dict[str, Any]], split_name: str) -> Dataset:
        normalized_records = []
        for obj in raw_data:
            rec = _normalize_record(obj, self.config.dataset_prompt_key, self.config.dataset_answer_key, self.system_prompt)
            if rec and not _looks_garbage(rec["prompt"]) and not _looks_garbage(rec["completion"]):
                if not self.config.dataset_filter_keywords or not (_contains_keywords(rec["prompt"], self.config.dataset_filter_keywords) or _contains_keywords(rec["completion"], self.config.dataset_filter_keywords)):
                    normalized_records.append(rec)
        if not normalized_records:
            logger.warning(f"No valid records found for {split_name}.")
            return Dataset.from_list([])
        
        features = Features({
            'prompt': Value('string'),
            'completion': Value('string'),
            'system': Value('string'),
            'test_cases': [Value('string')],
            'is_invalid_sample': Value('bool'),
            'meta': {
                'is_mcq': Value('bool'),
                'mcq_options': [Value('string')],
                'mcq_multi_select': Value('bool'),
                'mcq_correct_indices': [Value('int32')],
                'mcq_correct_letters': Value('string'),
            }
        })
        return Dataset.from_list(normalized_records, features=features)

    def get_dataloader(self, split: str, batch_size: int) -> Iterator[Dict[str, Any]]:
        dataset = self._train_dataset if split == 'train' else self._val_dataset
        if not dataset or len(dataset) == 0:
            logger.warning(f"Dataloader for '{split}' is empty."); return iter([])

        indices = list(range(len(dataset)))
        if self.config.shuffle_data and split == 'train': random.shuffle(indices)

        # Create a dummy ExperimentConfig to pass to build_rollout_batch,
        # as it expects the top-level config.
        dummy_exp_config = ExperimentConfig(
            trainer={'algorithm': 'grpo', 'output_dir': Path('./'), 'num_training_steps':1, 'learning_rate':1e-5, 'ppo_batch_size':1, 'num_rollout_samples':1, 'grad_accum_steps':1},
            model={'model_path': Path('./')},
            data=self.config
        )
        dummy_exp_config.system_prompt = self.system_prompt

        def batch_generator():
            for i in range(0, len(indices), batch_size):
                batch_indices = indices[i:i + batch_size]
                if not batch_indices: continue
                
                prompts_data, prompts_mx, _ = build_rollout_batch(self._tokenizer, dataset, batch_indices, dummy_exp_config)
                
                if prompts_mx.size > 0:
                    yield {
                        'prompts_data': prompts_data,
                        'prompts_mx': prompts_mx,
                    }
        return batch_generator()
EOF

echo "AttributeError in data/batch_builder.py has been fixed."
echo "Please re-run the training command."
